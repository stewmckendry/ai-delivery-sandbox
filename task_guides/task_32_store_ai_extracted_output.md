# Task 32: Store AI-Extracted Records from ETL Pipeline

## ðŸŽ¯ Goal
Persist the structured records generated by the LLM-based extractor + cleaner so they can be queried later by the app (e.g. summary, RAG).

## ðŸ“‚ Target Files
- `app/orchestrator.py`
- `app/storage/structured.py` (new)

## ðŸ“‹ Instructions
- Create a SQLite table:
```sql
StructuredRecord(id, portal, type, text, source_url, date_created)
```
- Write `insert_structured_records(session, records: list[dict])`
- In `run_etl_for_portal()`, after cleaner step:
```python
insert_structured_records(session, final_records)
```
- Log count of inserted records

## ðŸ§ª Test
- Add unit test for insert function
- Run test_portal and check DB for saved entries

## âœ… What to Report Back
- Insert function and table definition
- Updated orchestrator block
- Record examples saved from test_portal

This ensures both rule-based and AI-based records are persisted for reuse.