# Task 32: Store AI-Extracted Records from ETL Pipeline

## ðŸŽ¯ Goal
Persist the structured records generated by the LLM-based extractor + cleaner so they can be queried later by the app (e.g. summary, RAG).

## ðŸ“‚ Target Files
- `app/storage/models.py` â€“ add `StructuredRecord` table
- `app/storage/structured.py` (new) â€“ add insert helper
- `app/orchestrator.py` â€“ call insert function after cleaner

## ðŸ“‹ Instructions
- Define a new model:
```python
class StructuredRecord(Base):
    __tablename__ = "structured_records"
    id = Column(Integer, primary_key=True, index=True)
    portal = Column(String)
    type = Column(String)
    text = Column(Text)
    source_url = Column(String)
    date_created = Column(DateTime, default=datetime.utcnow)
```
- Create insert function:
```python
def insert_structured_records(session, records: list[dict]):
    objs = [StructuredRecord(**r) for r in records]
    session.add_all(objs)
    session.commit()
```
- In `run_etl_for_portal()`, after final_records:
```python
insert_structured_records(session, final_records)
```
- Log insert count

## ðŸ§ª Test
- Add test for inserting 2 records and verifying row count
- Run `test_portal` and inspect DB

## âœ… What to Report Back
- Model, helper, orchestrator update
- Sample inserted record

This enables long-term use of structured, AI-tagged records.