# ğŸ§  WP22 Addendum â€“ queryCorpus Remote Logic

## ğŸ¯ Objective
Support remote semantic querying against vector-embedded policy corpora hosted on Railway. In remote mode, queryCorpus returns **GPT-synthesized summaries** based on top-matching vector chunks retrieved from ChromaDB.

## âš™ï¸ Logic Overview
The tool dynamically switches behavior based on environment settings:
- If `CHROMA_SERVER_HOST` is defined â†’ use remote mode
- Otherwise â†’ fallback to local mode

### ğŸ” Inputs
```json
{
  "query": "open government policy objectives"
}
```

## ğŸ“¦ Libraries + Components

### ğŸ“š `chromadb.HttpClient`
- Connects to remote Chroma instance using host and port from env
- Retrieves most similar vector-embedded chunks using `collection.query(...)`

### ğŸ“– `langchain.prompts.PromptTemplate`
- Defines a text prompt for GPT to answer based on raw chunk content
- Template: "Based on the following documents... Answer the query..."

### ğŸ’¬ `langchain.chat_models.ChatOpenAI`
- Calls OpenAIâ€™s chat model to generate the final synthesized answer
- Same model and behavior used as in local mode

### ğŸ”— `langchain.chains.LLMChain`
- Binds prompt + model to a single callable object for query execution
- Returns structured response `{ "answer": <string>, "documents": [<chunks>] }`

---

## ğŸ§ª Remote Result Format
```json
{
  "answer": "Summary generated by GPT",
  "documents": ["Top matching chunk 1", "Chunk 2", ...]
}
```

---

## âœ… Benefit
This approach ensures queryCorpus provides the same synthesized output experience for both local development and cloud-deployed environments.