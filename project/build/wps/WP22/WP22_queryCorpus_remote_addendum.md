# 🧠 WP22 Addendum – queryCorpus Remote Logic

## 🎯 Objective
Support remote semantic querying against vector-embedded policy corpora hosted on Railway. In remote mode, queryCorpus returns **GPT-synthesized summaries** based on top-matching vector chunks retrieved from ChromaDB.

## ⚙️ Logic Overview
The tool dynamically switches behavior based on environment settings:
- If `CHROMA_SERVER_HOST` is defined → use remote mode
- Otherwise → fallback to local mode

### 🔐 Inputs
```json
{
  "query": "open government policy objectives"
}
```

## 📦 Libraries + Components

### 📚 `chromadb.HttpClient`
- Connects to remote Chroma instance using host and port from env
- Retrieves most similar vector-embedded chunks using `collection.query(...)`

### 📖 `langchain.prompts.PromptTemplate`
- Defines a text prompt for GPT to answer based on raw chunk content
- Template: "Based on the following documents... Answer the query..."

### 💬 `langchain.chat_models.ChatOpenAI`
- Calls OpenAI’s chat model to generate the final synthesized answer
- Same model and behavior used as in local mode

### 🔗 `langchain.chains.LLMChain`
- Binds prompt + model to a single callable object for query execution
- Returns structured response `{ "answer": <string>, "documents": [<chunks>] }`

---

## 🧪 Remote Result Format
```json
{
  "answer": "Summary generated by GPT",
  "documents": ["Top matching chunk 1", "Chunk 2", ...]
}
```

---

## ✅ Benefit
This approach ensures queryCorpus provides the same synthesized output experience for both local development and cloud-deployed environments.